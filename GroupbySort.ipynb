{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg,max\n\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n  ]\n\nschema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1c942fab-009a-41c4-b799-44d7364a47f2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NV   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |DE   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |NV   |80000 |25 |18000|\n|Kumar        |Marketing |NJ   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.groupBy(\"state\").sum(\"salary\").show()\n\ndfGroup=df.groupBy(\"state\") \\\n          .agg(sum(\"salary\").alias(\"sum_salary\"))\n          \ndfGroup.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4f6386c6-62f7-4dfc-b96f-0eb6fe155a4d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----------+\n|state|sum(salary)|\n+-----+-----------+\n|   NY|     252000|\n|   NV|     166000|\n|   CA|     171000|\n|   DE|      99000|\n|   NJ|      91000|\n+-----+-----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|NY   |252000    |\n|NV   |166000    |\n|CA   |171000    |\n|DE   |99000     |\n|NJ   |91000     |\n+-----+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["dfGroup.show(truncate=False)\n\ndfFilter=dfGroup.filter(dfGroup.sum_salary > 100000)\ndfFilter.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"58c88889-2bed-464a-bb31-ef65d622320c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|NY   |252000    |\n|NV   |166000    |\n|CA   |171000    |\n|DE   |99000     |\n|NJ   |91000     |\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n+-----+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import asc\ndfFilter.sort(\"sum_salary\").show()\n\nfrom pyspark.sql.functions import desc\ndfFilter.sort(desc(\"sum_salary\")).show()\n\ndf.groupBy(\"state\") \\\n  .agg(sum(\"salary\").alias(\"sum_salary\")) \\\n  .filter(col(\"sum_salary\") > 100000)  \\\n  .sort(desc(\"sum_salary\")) \\\n  .show()\n  \ndf.createOrReplaceTempView(\"EMP\")\nspark.sql(\"select state, sum(salary) as sum_salary from EMP \" +\n          \"group by state having sum_salary > 100000 \" + \n          \"order by sum_salary desc\").show()\n\ndf.groupBy(\"state\") \\\n  .sum(\"salary\") \\\n  .withColumnRenamed(\"sum(salary)\", \"sum_salary\") \\\n  .show()\n\ndf.groupBy(\"state\") \\\n  .sum(\"salary\") \\\n  .select(col(\"state\"),col(\"sum(salary)\").alias(\"sum_salary\")) \\\n  .show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b4e13b90-59c3-4484-b727-c60da15d101c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NV|    166000|\n|   CA|    171000|\n|   NY|    252000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n|   DE|     99000|\n|   NJ|     91000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n|   DE|     99000|\n|   NJ|     91000|\n+-----+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg,max\n\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n  ]\n\nschema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.groupBy(\"state\").sum(\"salary\").show()\n\ndfGroup=df.groupBy(\"state\") \\\n          .agg(sum(\"salary\").alias(\"sum_salary\"))\n          \ndfGroup.show(truncate=False)\n\ndfFilter=dfGroup.filter(dfGroup.sum_salary > 100000)\ndfFilter.show()\n\nfrom pyspark.sql.functions import asc\ndfFilter.sort(\"sum_salary\").show()\n\nfrom pyspark.sql.functions import desc\ndfFilter.sort(desc(\"sum_salary\")).show()\n\ndf.groupBy(\"state\") \\\n  .agg(sum(\"salary\").alias(\"sum_salary\")) \\\n  .filter(col(\"sum_salary\") > 100000)  \\\n  .sort(desc(\"sum_salary\")) \\\n  .show()\n  \ndf.createOrReplaceTempView(\"EMP\")\nspark.sql(\"select state, sum(salary) as sum_salary from EMP \" +\n          \"group by state having sum_salary > 100000 \" + \n          \"order by sum_salary desc\").show()\n\ndf.groupBy(\"state\") \\\n  .sum(\"salary\") \\\n  .withColumnRenamed(\"sum(salary)\", \"sum_salary\") \\\n  .show()\n\ndf.groupBy(\"state\") \\\n  .sum(\"salary\") \\\n  .select(col(\"state\"),col(\"sum(salary)\").alias(\"sum_salary\")) \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"874c8961-df2c-4395-b99c-9e8f6c7bdc5e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"GroupbySort","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
