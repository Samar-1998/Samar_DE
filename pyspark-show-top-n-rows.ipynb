{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\",34),(\"Ann\",34),\n    (\"Michael\",33),(\"Scott\",53),\n    (\"Robert\",37),(\"Chad\",27)\n  ]\n\ncolumns = [\"firstname\",\"age\",]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\n\n\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"90199226-4c64-4bc0-aaf4-d5eec8b396b6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---+\n|firstname|age|\n+---------+---+\n|    James| 34|\n|      Ann| 34|\n|  Michael| 33|\n|    Scott| 53|\n|   Robert| 37|\n|     Chad| 27|\n+---------+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["print(df.take(2))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e2cc853f-f8bd-488a-8938-1cb6dee4c44b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[Row(firstname='James', age=34), Row(firstname='Ann', age=34)]\n"]}],"execution_count":0},{"cell_type":"code","source":["print(df.tail(2))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c9aff4da-f374-4b63-a7f2-9d0b9d7bad87","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[Row(firstname='Robert', age=37), Row(firstname='Chad', age=27)]\n"]}],"execution_count":0},{"cell_type":"code","source":["print(df.head(2))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5bdc6172-010a-4d69-81ad-cc5fc550c168","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[Row(firstname='James', age=34), Row(firstname='Ann', age=34)]\n"]}],"execution_count":0},{"cell_type":"code","source":["print(df.first())\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"170a3793-083d-4ac2-bc06-d08a37590869","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Row(firstname='James', age=34)\n"]}],"execution_count":0},{"cell_type":"code","source":["print(df.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0b2ff907-1400-4497-b139-27477c3d89f6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[Row(firstname='James', age=34), Row(firstname='Ann', age=34), Row(firstname='Michael', age=33), Row(firstname='Scott', age=53), Row(firstname='Robert', age=37), Row(firstname='Chad', age=27)]\n"]}],"execution_count":0},{"cell_type":"code","source":["pandasDF=df.limit(3).toPandas()\nprint(pandasDF)\n\n\n#\"Limits the result count to the number specified.\n#Returns a new Dataset by taking the first n rows."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b5b1a1cd-0eed-4b98-9a83-ef1bae153e62","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["  firstname  age\n0     James   34\n1       Ann   34\n2   Michael   33\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\",34),(\"Ann\",34),\n    (\"Michael\",33),(\"Scott\",53),\n    (\"Robert\",37),(\"Chad\",27)\n  ]\n\ncolumns = [\"firstname\",\"age\",]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\n\n\ndf.show()\n#Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n# Internally calls limit and collect\n#Action, Return Array[T]\nprint(df.take(2))\n\n#Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n#Running tail requires moving data into the application's driver process, and doing so with\n#a very large ``num`` can crash the driver process with OutOfMemoryError.\n#Return Array[T]\nprint(df.tail(2))\n\n\n\"\"\"Returns the first ``n`` rows.\n\n.. note:: This method should only be used if the resulting array is expected\n    to be small, as all the data is loaded into the driver's memory.\n\n:param n: int, default 1. Number of rows to return.\n:return: If n is greater than 1, return a list of :class:`Row`.\n    If n is 1, return a single Row.\"\"\"\n#Return Array[T]\nprint(df.head(2))\n\n\n#Returns the first row, same as df.head(1)\nprint(df.first())\n\n#Returns all the records as a list of :class:`Row`.\n#Action, Return Array[T]\nprint(df.collect())\n#\"Limits the result count to the number specified.\n#Returns a new Dataset by taking the first n rows.\npandasDF=df.limit(3).toPandas()\nprint(pandasDF)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2546e213-4bb5-42a3-8b58-0a1d1c36a4ba","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-show-top-n-rows","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
