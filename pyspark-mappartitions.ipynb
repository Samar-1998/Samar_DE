{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [('James','Smith','M',3000),\n  ('Anna','Rose','F',4100),\n  ('Robert','Williams','M',6200), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"eb633d12-f2b7-4f4f-bc2e-8641248841b0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Example 1 mapPartitions()\ndef reformat(partitionData):\n    for row in partitionData:\n        yield [row.firstname+\",\"+row.lastname,row.salary*10/100]\ndf.rdd.mapPartitions(reformat).toDF().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8f716444-316c-490f-be64-5ea84211e681","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------------+-----+\n|             _1|   _2|\n+---------------+-----+\n|    James,Smith|300.0|\n|      Anna,Rose|410.0|\n|Robert,Williams|620.0|\n+---------------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [('James','Smith','M',3000),\n  ('Anna','Rose','F',4100),\n  ('Robert','Williams','M',6200), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n\n#Example 1 mapPartitions()\ndef reformat(partitionData):\n    for row in partitionData:\n        yield [row.firstname+\",\"+row.lastname,row.salary*10/100]\ndf.rdd.mapPartitions(reformat).toDF().show()\n\n#Example 2 mapPartitions()\ndef reformat2(partitionData):\n  updatedData = []\n  for row in partitionData:\n    name=row.firstname+\",\"+row.lastname\n    bonus=row.salary*10/100\n    updatedData.append([name,bonus])\n  return iter(updatedData)\n\ndf2=df.rdd.mapPartitions(reformat2).toDF(\"name\",\"bonus\")\ndf2.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"176a1eed-c928-45ff-8989-6803df79f717","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-mappartitions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
