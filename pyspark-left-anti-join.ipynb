{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('machinelearninggeeks.com').getOrCreate()\n  \nemp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n  ]\nempColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n       \"emp_dept_id\",\"gender\",\"salary\"]\n\nempDF = spark.createDataFrame(data=emp, schema = empColumns)\nempDF.printSchema()\nempDF.show(truncate=False)\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"71eaed26-2529-4b34-aed5-14b527961a91","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d6ba8179-0461-4fcd-b079-a30982a3c1f8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["empDF.createOrReplaceTempView(\"EMP\")\ndeptDF.createOrReplaceTempView(\"DEPT\")\n\njoinDF2 = spark.sql(\"SELECT e.* FROM EMP e LEFT ANTI JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n  .show(truncate=False) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"612f12bd-3e1e-4124-a8b4-684035e35405","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+-----+---------------+-----------+-----------+------+------+\n|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|6     |Brown|2              |2010       |50         |      |-1    |\n+------+-----+---------------+-----------+-----------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('machinelearninggeeks.com').getOrCreate()\n  \nemp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n  ]\nempColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n       \"emp_dept_id\",\"gender\",\"salary\"]\n\nempDF = spark.createDataFrame(data=emp, schema = empColumns)\nempDF.printSchema()\nempDF.show(truncate=False)\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\").show(truncate=False)\n\nempDF.createOrReplaceTempView(\"EMP\")\ndeptDF.createOrReplaceTempView(\"DEPT\")\n\njoinDF2 = spark.sql(\"SELECT e.* FROM EMP e LEFT ANTI JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n  .show(truncate=False) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a233ce6c-8725-46b5-ae67-4009f77eac41","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-left-anti-join","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
