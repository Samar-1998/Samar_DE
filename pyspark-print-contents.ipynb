{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\n\nrdd=spark.sparkContext.parallelize(dept)\nprint(rdd)\ndataColl=rdd.collect()\n\nfor row in dataColl:\n    print(row[0] + \",\" +str(row[1]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"dda38de1-db0a-43cc-bae2-1d3b5e05ba09","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["ParallelCollectionRDD[579] at readRDDFromInputStream at PythonRDD.scala:435\nFinance,10\nMarketing,20\nSales,30\nIT,40\n"]}],"execution_count":0},{"cell_type":"code","source":["deptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\ndataCollect = deptDF.collect()\n\nprint(dataCollect)\n\ndataCollect2 = deptDF.select(\"dept_name\").collect()\nprint(dataCollect2)\n\nfor row in dataCollect:\n    print(row['dept_name'] + \",\" +str(row['dept_id']))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ac4ef9c1-1257-46bf-a0cd-a5c82ed97a81","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n[Row(dept_name='Finance'), Row(dept_name='Marketing'), Row(dept_name='Sales'), Row(dept_name='IT')]\nFinance,10\nMarketing,20\nSales,30\nIT,40\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\n\nrdd=spark.sparkContext.parallelize(dept)\nprint(rdd)\ndataColl=rdd.collect()\n\nfor row in dataColl:\n    print(row[0] + \",\" +str(row[1]))\n\"\"\"\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\ndataCollect = deptDF.collect()\n\nprint(dataCollect)\n\ndataCollect2 = deptDF.select(\"dept_name\").collect()\nprint(dataCollect2)\n\nfor row in dataCollect:\n    print(row['dept_name'] + \",\" +str(row['dept_id']))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4cff4cf2-eb5c-4211-a35b-1d5a5966e2b6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-print-contents","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
