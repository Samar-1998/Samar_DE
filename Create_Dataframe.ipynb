{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nfrom pyspark.sql.functions import *\n\ncolumns = [\"language\",\"users_count\"]\ndata = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1ad8639d-e32b-4709-b87a-10c34d2bd4b1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dfFromRDD1 = rdd.toDF()\ndfFromRDD1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0586ab84-827a-43c3-a930-4905265dba4f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["dfFromRDD1 = rdd.toDF(columns)\ndfFromRDD1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6ae6f53e-9222-4950-890c-1e4ed43ff0ad","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\ndfFromRDD2.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"454f64db-0d5a-4db3-a0b5-a80e06da14ca","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["dfFromData2 = spark.createDataFrame(data).toDF(*columns)\ndfFromData2.printSchema()    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c63b619c-4af0-40d4-a324-e87e4a904130","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["rowData = map(lambda x: Row(*x), data) \ndfFromData3 = spark.createDataFrame(rowData,columns)\ndfFromData3.printSchema()\ndfFromData3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b1e31d5d-58b0-46dc-90a2-4315c338fd16","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n+--------+-----------+\n|language|users_count|\n+--------+-----------+\n|    Java|      20000|\n|  Python|     100000|\n|   Scala|       3000|\n+--------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nfrom pyspark.sql.functions import *\n\ncolumns = [\"language\",\"users_count\"]\ndata = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)\n\ndfFromRDD1 = rdd.toDF()\ndfFromRDD1.printSchema()\n\ndfFromRDD1 = rdd.toDF(columns)\ndfFromRDD1.printSchema()\n\ndfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\ndfFromRDD2.printSchema()\n\ndfFromData2 = spark.createDataFrame(data).toDF(*columns)\ndfFromData2.printSchema()     \n\nrowData = map(lambda x: Row(*x), data) \ndfFromData3 = spark.createDataFrame(rowData,columns)\ndfFromData3.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0cdb68d0-0f36-4475-aafb-cb4f185389da","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Create_Dataframe","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
