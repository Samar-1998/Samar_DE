{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, col\nspark=SparkSession.builder.appName(\"sparkbyexamples\").getOrCreate()\n\ndata=data = [('James','','Smith','1991-04-01'),\n  ('Michael','Rose','','2000-05-19'),\n  ('Robert','','Williams','1978-09-05'),\n  ('Maria','Anne','Jones','1967-12-01'),\n  ('Jen','Mary','Brown','1980-02-17')\n]\n\ncolumns=[\"firstname\",\"middlename\",\"lastname\",\"dob\"]\ndf=spark.createDataFrame(data,columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c9d61298-30fa-4df0-865d-06df52a04137","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n\n+---------+----------+--------+----------+\n|firstname|middlename|lastname|dob       |\n+---------+----------+--------+----------+\n|James    |          |Smith   |1991-04-01|\n|Michael  |Rose      |        |2000-05-19|\n|Robert   |          |Williams|1978-09-05|\n|Maria    |Anne      |Jones   |1967-12-01|\n|Jen      |Mary      |Brown   |1980-02-17|\n+---------+----------+--------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df1 = df.withColumn('year', split(df['dob'], '-').getItem(0)) \\\n       .withColumn('month', split(df['dob'], '-').getItem(1)) \\\n       .withColumn('day', split(df['dob'], '-').getItem(2))\ndf1.printSchema()\ndf1.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"22683a43-b606-481d-896e-af18791ea177","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- year: string (nullable = true)\n |-- month: string (nullable = true)\n |-- day: string (nullable = true)\n\n+---------+----------+--------+----------+----+-----+---+\n|firstname|middlename|lastname|dob       |year|month|day|\n+---------+----------+--------+----------+----+-----+---+\n|James    |          |Smith   |1991-04-01|1991|04   |01 |\n|Michael  |Rose      |        |2000-05-19|2000|05   |19 |\n|Robert   |          |Williams|1978-09-05|1978|09   |05 |\n|Maria    |Anne      |Jones   |1967-12-01|1967|12   |01 |\n|Jen      |Mary      |Brown   |1980-02-17|1980|02   |17 |\n+---------+----------+--------+----------+----+-----+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[" # Alternatively we can do like below      \nsplit_col = pyspark.sql.functions.split(df['dob'], '-')\ndf2 = df.withColumn('year', split_col.getItem(0)) \\\n       .withColumn('month', split_col.getItem(1)) \\\n       .withColumn('day', split_col.getItem(2))\ndf2.show(truncate=False)  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"858ea848-9032-49ed-a136-ab8c12fb042c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+----------+--------+----------+----+-----+---+\n|firstname|middlename|lastname|dob       |year|month|day|\n+---------+----------+--------+----------+----+-----+---+\n|James    |          |Smith   |1991-04-01|1991|04   |01 |\n|Michael  |Rose      |        |2000-05-19|2000|05   |19 |\n|Robert   |          |Williams|1978-09-05|1978|09   |05 |\n|Maria    |Anne      |Jones   |1967-12-01|1967|12   |01 |\n|Jen      |Mary      |Brown   |1980-02-17|1980|02   |17 |\n+---------+----------+--------+----------+----+-----+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["split_col = pyspark.sql.functions.split(df['dob'], '-')\ndf3 = df.select(\"firstname\",\"middlename\",\"lastname\",\"dob\", split_col.getItem(0).alias('year'),split_col.getItem(1).alias('month'),split_col.getItem(2).alias('day'))   \ndf3.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1109f375-aec4-4432-ae9f-86bbfeaa584d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+----------+--------+----------+----+-----+---+\n|firstname|middlename|lastname|dob       |year|month|day|\n+---------+----------+--------+----------+----+-----+---+\n|James    |          |Smith   |1991-04-01|1991|04   |01 |\n|Michael  |Rose      |        |2000-05-19|2000|05   |19 |\n|Robert   |          |Williams|1978-09-05|1978|09   |05 |\n|Maria    |Anne      |Jones   |1967-12-01|1967|12   |01 |\n|Jen      |Mary      |Brown   |1980-02-17|1980|02   |17 |\n+---------+----------+--------+----------+----+-----+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, col\nspark=SparkSession.builder.appName(\"sparkbyexamples\").getOrCreate()\n\ndata=data = [('James','','Smith','1991-04-01'),\n  ('Michael','Rose','','2000-05-19'),\n  ('Robert','','Williams','1978-09-05'),\n  ('Maria','Anne','Jones','1967-12-01'),\n  ('Jen','Mary','Brown','1980-02-17')\n]\n\ncolumns=[\"firstname\",\"middlename\",\"lastname\",\"dob\"]\ndf=spark.createDataFrame(data,columns)\ndf.printSchema()\ndf.show(truncate=False)\ndf1 = df.withColumn('year', split(df['dob'], '-').getItem(0)) \\\n       .withColumn('month', split(df['dob'], '-').getItem(1)) \\\n       .withColumn('day', split(df['dob'], '-').getItem(2))\ndf1.printSchema()\ndf1.show(truncate=False)\n\n # Alternatively we can do like below      \nsplit_col = pyspark.sql.functions.split(df['dob'], '-')\ndf2 = df.withColumn('year', split_col.getItem(0)) \\\n       .withColumn('month', split_col.getItem(1)) \\\n       .withColumn('day', split_col.getItem(2))\ndf2.show(truncate=False)      \n\n# Using split() function of Column class\nsplit_col = pyspark.sql.functions.split(df['dob'], '-')\ndf3 = df.select(\"firstname\",\"middlename\",\"lastname\",\"dob\", split_col.getItem(0).alias('year'),split_col.getItem(1).alias('month'),split_col.getItem(2).alias('day'))   \ndf3.show(truncate=False)\n\n\"\"\"\ndf4=spark.createDataFrame([(\"20-13-2012-monday\",)], ['date',])\n\ndf4.select(split(df4.date,'^([\\d]+-[\\d]+-[\\d])').alias('date'),\n    regexp_replace(split(df4.date,'^([\\d]+-[\\d]+-[\\d]+)').getItem(1),'-','').alias('day')).show()\n    \"\"\"\ndf4 = spark.createDataFrame([('oneAtwoBthree',)], ['str',])\ndf4.select(split(df4.str, '[AB]').alias('str')).show()\n\ndf4.select(split(df4.str, '[AB]',2).alias('str')).show()\ndf4.select(split(df4.str, '[AB]',1).alias('str')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a431568b-bf84-41e3-b12e-c98eb69389f4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-split-function","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
