{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\ninputRDD = spark.sparkContext.parallelize(data)\n  \nlistRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0c92dcc2-845e-4423-ac75-c927cb09d2f1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#aggregate\nseqOp = (lambda x, y: x + y)\ncombOp = (lambda x, y: x + y)\nagg=listRdd.aggregate(0, seqOp, combOp)\nprint(agg) # output 20\n\n#aggregate 2\nseqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))\ncombOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\nagg2=listRdd.aggregate((0, 0), seqOp2, combOp2)\nprint(agg2) # output (20,7)\n\nagg2=listRdd.treeAggregate(0,seqOp, combOp)\nprint(agg2) # output 20"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"15fb395d-1d97-49d0-ab78-ee7d8fae8af5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["20\n(20, 7)\n20\n"]}],"execution_count":0},{"cell_type":"code","source":["#fold\nfrom operator import add\nfoldRes=listRdd.fold(0, add)\nprint(foldRes) # output 20\n\n#reduce\nredRes=listRdd.reduce(add)\nprint(redRes) # output 20\n\n#treeReduce. This is similar to reduce\nadd = lambda x, y: x + y\nredRes=listRdd.treeReduce(add)\nprint(redRes) # output 20\n\n#Collect\ndata = listRdd.collect()\nprint(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cc9086c7-8b01-457d-8e01-277d059c0948","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["20\n20\n20\n[1, 2, 3, 4, 5, 3, 2]\n"]}],"execution_count":0},{"cell_type":"code","source":["#count, countApprox, countApproxDistinct\nprint(\"Count : \"+str(listRdd.count()))\n#Output: Count : 20\nprint(\"countApprox : \"+str(listRdd.countApprox(1200)))\n#Output: countApprox : (final: [7.000, 7.000])\nprint(\"countApproxDistinct : \"+str(listRdd.countApproxDistinct()))\n#Output: countApproxDistinct : 5\nprint(\"countApproxDistinct : \"+str(inputRDD.countApproxDistinct()))\n#Output: countApproxDistinct : 5\n\n#countByValue, countByValueApprox\nprint(\"countByValue :  \"+str(listRdd.countByValue()))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"877b737c-b467-41ed-a8bb-7eb408ab8e33","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Count : 7\ncountApprox : 7\ncountApproxDistinct : 5\ncountApproxDistinct : 5\ncountByValue :  defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 2, 4: 1, 5: 1})\n"]}],"execution_count":0},{"cell_type":"code","source":["#first\nprint(\"first :  \"+str(listRdd.first()))\n#Output: first :  1\nprint(\"first :  \"+str(inputRDD.first()))\n#Output: first :  (Z,1)\n\n#top\nprint(\"top : \"+str(listRdd.top(2)))\n#Output: take : 5,4\nprint(\"top : \"+str(inputRDD.top(2)))\n#Output: take : (Z,1),(C,40)\n\n#min\nprint(\"min :  \"+str(listRdd.min()))\n#Output: min :  1\nprint(\"min :  \"+str(inputRDD.min()))\n#Output: min :  (A,20)  \n\n#max\nprint(\"max :  \"+str(listRdd.max()))\n#Output: max :  5\nprint(\"max :  \"+str(inputRDD.max()))\n#Output: max :  (Z,1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"dc9fbd9b-6647-4ef4-9970-c0ce4ec15fd3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["first :  1\nfirst :  ('Z', 1)\ntop : [5, 4]\ntop : [('Z', 1), ('C', 40)]\nmin :  1\nmin :  ('A', 20)\nmax :  5\nmax :  ('Z', 1)\n"]}],"execution_count":0},{"cell_type":"code","source":["#take, takeOrdered, takeSample\nprint(\"take : \"+str(listRdd.take(2)))\n#Output: take : 1,2\nprint(\"takeOrdered : \"+ str(listRdd.takeOrdered(2)))\n#Output: takeOrdered : 1,2\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a8a9a3d2-ea3e-43bf-83bd-12aa545ea997","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["take : [1, 2]\ntakeOrdered : [1, 2]\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\ninputRDD = spark.sparkContext.parallelize(data)\n  \nlistRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])\n\n#aggregate\nseqOp = (lambda x, y: x + y)\ncombOp = (lambda x, y: x + y)\nagg=listRdd.aggregate(0, seqOp, combOp)\nprint(agg) # output 20\n\n#aggregate 2\nseqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))\ncombOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\nagg2=listRdd.aggregate((0, 0), seqOp2, combOp2)\nprint(agg2) # output (20,7)\n\nagg2=listRdd.treeAggregate(0,seqOp, combOp)\nprint(agg2) # output 20\n\n#fold\nfrom operator import add\nfoldRes=listRdd.fold(0, add)\nprint(foldRes) # output 20\n\n#reduce\nredRes=listRdd.reduce(add)\nprint(redRes) # output 20\n\n#treeReduce. This is similar to reduce\nadd = lambda x, y: x + y\nredRes=listRdd.treeReduce(add)\nprint(redRes) # output 20\n\n#Collect\ndata = listRdd.collect()\nprint(data)\n\n#count, countApprox, countApproxDistinct\nprint(\"Count : \"+str(listRdd.count()))\n#Output: Count : 20\nprint(\"countApprox : \"+str(listRdd.countApprox(1200)))\n#Output: countApprox : (final: [7.000, 7.000])\nprint(\"countApproxDistinct : \"+str(listRdd.countApproxDistinct()))\n#Output: countApproxDistinct : 5\nprint(\"countApproxDistinct : \"+str(inputRDD.countApproxDistinct()))\n#Output: countApproxDistinct : 5\n\n#countByValue, countByValueApprox\nprint(\"countByValue :  \"+str(listRdd.countByValue()))\n\n\n#first\nprint(\"first :  \"+str(listRdd.first()))\n#Output: first :  1\nprint(\"first :  \"+str(inputRDD.first()))\n#Output: first :  (Z,1)\n\n#top\nprint(\"top : \"+str(listRdd.top(2)))\n#Output: take : 5,4\nprint(\"top : \"+str(inputRDD.top(2)))\n#Output: take : (Z,1),(C,40)\n\n#min\nprint(\"min :  \"+str(listRdd.min()))\n#Output: min :  1\nprint(\"min :  \"+str(inputRDD.min()))\n#Output: min :  (A,20)  \n\n#max\nprint(\"max :  \"+str(listRdd.max()))\n#Output: max :  5\nprint(\"max :  \"+str(inputRDD.max()))\n#Output: max :  (Z,1)\n\n#take, takeOrdered, takeSample\nprint(\"take : \"+str(listRdd.take(2)))\n#Output: take : 1,2\nprint(\"takeOrdered : \"+ str(listRdd.takeOrdered(2)))\n#Output: takeOrdered : 1,2\nprint(\"take : \"+str(listRdd.takeSample()))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1d4dd5f-9167-45c5-b35b-b1940bdef348","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-rdd-actions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
