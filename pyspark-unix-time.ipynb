{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\n\ninputData = [(\"2019-07-01 12:01:19\",\n            \"07-01-2019 12:01:19\", \n            \"07-01-2019\")]\ncolumns=[\"timestamp_1\",\"timestamp_2\",\"timestamp_3\"]\ndf=spark.createDataFrame(\n        data = inputData,\n        schema = columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e31232aa-082e-4424-afaa-55a4df87ecaa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- timestamp_1: string (nullable = true)\n |-- timestamp_2: string (nullable = true)\n |-- timestamp_3: string (nullable = true)\n\n+-------------------+-------------------+-----------+\n|timestamp_1        |timestamp_2        |timestamp_3|\n+-------------------+-------------------+-----------+\n|2019-07-01 12:01:19|07-01-2019 12:01:19|07-01-2019 |\n+-------------------+-------------------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndf2 = df.select( \n      unix_timestamp(col(\"timestamp_1\")).alias(\"timestamp_1\"), \n      unix_timestamp(col(\"timestamp_2\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"), \n      unix_timestamp(col(\"timestamp_3\"),\"MM-dd-yyyy\").alias(\"timestamp_3\"), \n      unix_timestamp().alias(\"timestamp_4\") \n   )\ndf2.printSchema()\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ed4f2575-f6ee-4d8f-9bcf-7016050541d9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- timestamp_1: long (nullable = true)\n |-- timestamp_2: long (nullable = true)\n |-- timestamp_3: long (nullable = true)\n |-- timestamp_4: long (nullable = true)\n\n+-----------+-----------+-----------+-----------+\n|timestamp_1|timestamp_2|timestamp_3|timestamp_4|\n+-----------+-----------+-----------+-----------+\n|1561982479 |1561982479 |1561939200 |1687018599 |\n+-----------+-----------+-----------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df3=df2.select(\n    from_unixtime(col(\"timestamp_1\")).alias(\"timestamp_1\"),\n    from_unixtime(col(\"timestamp_2\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"),\n    from_unixtime(col(\"timestamp_3\"),\"MM-dd-yyyy\").alias(\"timestamp_3\"),\n    from_unixtime(col(\"timestamp_4\")).alias(\"timestamp_4\")\n  )\ndf3.printSchema()\ndf3.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a5620793-e33f-47b7-a7c3-33a266fa1bec","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- timestamp_1: string (nullable = true)\n |-- timestamp_2: string (nullable = true)\n |-- timestamp_3: string (nullable = true)\n |-- timestamp_4: string (nullable = true)\n\n+-------------------+-------------------+-----------+-------------------+\n|timestamp_1        |timestamp_2        |timestamp_3|timestamp_4        |\n+-------------------+-------------------+-----------+-------------------+\n|2019-07-01 12:01:19|07-01-2019 12:01:19|07-01-2019 |2023-06-17 16:18:55|\n+-------------------+-------------------+-----------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\n\ninputData = [(\"2019-07-01 12:01:19\",\n            \"07-01-2019 12:01:19\", \n            \"07-01-2019\")]\ncolumns=[\"timestamp_1\",\"timestamp_2\",\"timestamp_3\"]\ndf=spark.createDataFrame(\n        data = inputData,\n        schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import *\ndf2 = df.select( \n      unix_timestamp(col(\"timestamp_1\")).alias(\"timestamp_1\"), \n      unix_timestamp(col(\"timestamp_2\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"), \n      unix_timestamp(col(\"timestamp_3\"),\"MM-dd-yyyy\").alias(\"timestamp_3\"), \n      unix_timestamp().alias(\"timestamp_4\") \n   )\ndf2.printSchema()\ndf2.show(truncate=False)\n\ndf3=df2.select(\n    from_unixtime(col(\"timestamp_1\")).alias(\"timestamp_1\"),\n    from_unixtime(col(\"timestamp_2\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"),\n    from_unixtime(col(\"timestamp_3\"),\"MM-dd-yyyy\").alias(\"timestamp_3\"),\n    from_unixtime(col(\"timestamp_4\")).alias(\"timestamp_4\")\n  )\ndf3.printSchema()\ndf3.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6cb1b11-0894-424c-9caf-2849c3833f7c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-unix-time","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
