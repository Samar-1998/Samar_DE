{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nspark = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\ndata = [\n    (\"James\",None,\"M\"),\n    (\"Anna\",\"NY\",\"F\"),\n    (\"Julia\",None,None)\n  ]\n\ncolumns = [\"name\",\"state\",\"gender\"]\ndf = spark.createDataFrame(data,columns)\ndf.show()\n\ndf.filter(\"state is NULL\").show()\ndf.filter(df.state.isNull()).show()\ndf.filter(col(\"state\").isNull()).show() \n\ndf.na.drop(\"state\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"aeaa7e38-d232-4557-aab6-ea2953349149","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n| Anna|   NY|     F|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-856440254585859>:22\u001B[0m\n\u001B[1;32m     19\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(df\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     20\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow() \n\u001B[0;32m---> 22\u001B[0m df\u001B[38;5;241m.\u001B[39mna\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:5268\u001B[0m, in \u001B[0;36mDataFrameNaFunctions.drop\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n\u001B[1;32m   5262\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[1;32m   5263\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   5264\u001B[0m     how: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5265\u001B[0m     thresh: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   5266\u001B[0m     subset: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], List[\u001B[38;5;28mstr\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   5267\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[0;32m-> 5268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropna\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthresh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthresh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubset\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3968\u001B[0m, in \u001B[0;36mDataFrame.dropna\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n\u001B[1;32m   3925\u001B[0m \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` omitting rows with null values.\u001B[39;00m\n\u001B[1;32m   3926\u001B[0m \u001B[38;5;124;03m:func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\u001B[39;00m\n\u001B[1;32m   3927\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3965\u001B[0m \u001B[38;5;124;03m+---+------+-----+\u001B[39;00m\n\u001B[1;32m   3966\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3967\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m how \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m-> 3968\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m how \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) should be \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3970\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3971\u001B[0m     subset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n\n\u001B[0;31mValueError\u001B[0m: how ('state') should be 'any' or 'all'","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: how ('state') should be 'any' or 'all'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n","File \u001B[0;32m<command-856440254585859>:22\u001B[0m\n","\u001B[1;32m     19\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(df\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow()\n","\u001B[1;32m     20\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow() \n","\u001B[0;32m---> 22\u001B[0m df\u001B[38;5;241m.\u001B[39mna\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:5268\u001B[0m, in \u001B[0;36mDataFrameNaFunctions.drop\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n","\u001B[1;32m   5262\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n","\u001B[1;32m   5263\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n","\u001B[1;32m   5264\u001B[0m     how: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n","\u001B[1;32m   5265\u001B[0m     thresh: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n","\u001B[1;32m   5266\u001B[0m     subset: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], List[\u001B[38;5;28mstr\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n","\u001B[1;32m   5267\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n","\u001B[0;32m-> 5268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropna\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthresh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthresh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubset\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n","\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n","\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n","\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n","\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n","\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3968\u001B[0m, in \u001B[0;36mDataFrame.dropna\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n","\u001B[1;32m   3925\u001B[0m \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` omitting rows with null values.\u001B[39;00m\n","\u001B[1;32m   3926\u001B[0m \u001B[38;5;124;03m:func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\u001B[39;00m\n","\u001B[1;32m   3927\u001B[0m \n","\u001B[0;32m   (...)\u001B[0m\n","\u001B[1;32m   3965\u001B[0m \u001B[38;5;124;03m+---+------+-----+\u001B[39;00m\n","\u001B[1;32m   3966\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n","\u001B[1;32m   3967\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m how \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n","\u001B[0;32m-> 3968\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m how \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) should be \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n","\u001B[1;32m   3970\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n","\u001B[1;32m   3971\u001B[0m     subset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n","\n","\u001B[0;31mValueError\u001B[0m: how ('state') should be 'any' or 'all'"]}}],"execution_count":0},{"cell_type":"code","source":["df.na.drop(df.state).show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e8d2aabf-589f-455b-b395-1607c102509d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-856440254585862>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mna\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:5268\u001B[0m, in \u001B[0;36mDataFrameNaFunctions.drop\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n\u001B[1;32m   5262\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[1;32m   5263\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   5264\u001B[0m     how: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5265\u001B[0m     thresh: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   5266\u001B[0m     subset: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], List[\u001B[38;5;28mstr\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   5267\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[0;32m-> 5268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropna\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthresh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthresh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubset\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3967\u001B[0m, in \u001B[0;36mDataFrame.dropna\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n\u001B[1;32m   3919\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdropna\u001B[39m(\n\u001B[1;32m   3920\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   3921\u001B[0m     how: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3922\u001B[0m     thresh: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   3923\u001B[0m     subset: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], List[\u001B[38;5;28mstr\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   3924\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3925\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` omitting rows with null values.\u001B[39;00m\n\u001B[1;32m   3926\u001B[0m \u001B[38;5;124;03m    :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\u001B[39;00m\n\u001B[1;32m   3927\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3965\u001B[0m \u001B[38;5;124;03m    +---+------+-----+\u001B[39;00m\n\u001B[1;32m   3966\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3967\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[43mhow\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43many\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mall\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m:\n\u001B[1;32m   3968\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m how \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) should be \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3970\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:1295\u001B[0m, in \u001B[0;36mColumn.__nonzero__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1294\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__nonzero__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1295\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1296\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot convert column into bool: please use \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m&\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mand\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mor\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1297\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m~\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnot\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m when building DataFrame boolean expressions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1298\u001B[0m     )\n\n\u001B[0;31mValueError\u001B[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n","File \u001B[0;32m<command-856440254585862>:1\u001B[0m\n","\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mna\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:5268\u001B[0m, in \u001B[0;36mDataFrameNaFunctions.drop\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n","\u001B[1;32m   5262\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n","\u001B[1;32m   5263\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n","\u001B[1;32m   5264\u001B[0m     how: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n","\u001B[1;32m   5265\u001B[0m     thresh: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n","\u001B[1;32m   5266\u001B[0m     subset: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], List[\u001B[38;5;28mstr\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n","\u001B[1;32m   5267\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n","\u001B[0;32m-> 5268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropna\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthresh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthresh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubset\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n","\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n","\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n","\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n","\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n","\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3967\u001B[0m, in \u001B[0;36mDataFrame.dropna\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n","\u001B[1;32m   3919\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdropna\u001B[39m(\n","\u001B[1;32m   3920\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n","\u001B[1;32m   3921\u001B[0m     how: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n","\u001B[1;32m   3922\u001B[0m     thresh: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n","\u001B[1;32m   3923\u001B[0m     subset: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], List[\u001B[38;5;28mstr\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n","\u001B[1;32m   3924\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n","\u001B[1;32m   3925\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` omitting rows with null values.\u001B[39;00m\n","\u001B[1;32m   3926\u001B[0m \u001B[38;5;124;03m    :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\u001B[39;00m\n","\u001B[1;32m   3927\u001B[0m \n","\u001B[0;32m   (...)\u001B[0m\n","\u001B[1;32m   3965\u001B[0m \u001B[38;5;124;03m    +---+------+-----+\u001B[39;00m\n","\u001B[1;32m   3966\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n","\u001B[0;32m-> 3967\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[43mhow\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43many\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mall\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m:\n","\u001B[1;32m   3968\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m how \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) should be \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n","\u001B[1;32m   3970\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:1295\u001B[0m, in \u001B[0;36mColumn.__nonzero__\u001B[0;34m(self)\u001B[0m\n","\u001B[1;32m   1294\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__nonzero__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n","\u001B[0;32m-> 1295\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n","\u001B[1;32m   1296\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot convert column into bool: please use \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m&\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mand\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mor\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n","\u001B[1;32m   1297\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m~\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnot\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m when building DataFrame boolean expressions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n","\u001B[1;32m   1298\u001B[0m     )\n","\n","\u001B[0;31mValueError\u001B[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."]}}],"execution_count":0},{"cell_type":"code","source":["df.na.drop(subset=[\"state\"]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"22fc2fc0-61fc-490b-b692-00298333a04d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12284960-ec01-471f-ab04-6bc1bd111a05","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Filter_Null_1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
